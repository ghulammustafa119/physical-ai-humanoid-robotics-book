"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[455],{8425(e,n,i){i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"isaac/chapters/chapter3","title":"Chapter 3: Planning & Decision Making","description":"Task planning, motion planning, and decision-making for humanoid robots","source":"@site/docs/isaac/chapters/chapter3.md","sourceDirName":"isaac/chapters","slug":"/isaac/chapters/chapter3","permalink":"/physical-ai-humanoid-robotics-book/docs/isaac/chapters/chapter3","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Chapter 3: Planning & Decision Making","description":"Task planning, motion planning, and decision-making for humanoid robots","sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Perception in Physical AI Systems","permalink":"/physical-ai-humanoid-robotics-book/docs/isaac/chapters/chapter2"},"next":{"title":"Chapter 4: Bridging the AI Brain to ROS 2","permalink":"/physical-ai-humanoid-robotics-book/docs/isaac/chapters/chapter4"}}');var s=i(4848),t=i(8453);const o={title:"Chapter 3: Planning & Decision Making",description:"Task planning, motion planning, and decision-making for humanoid robots",sidebar_position:11},l="Chapter 3: Planning & Decision Making",r={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Types of Planning",id:"types-of-planning",level:2},{value:"Motion Planning",id:"motion-planning",level:3},{value:"Task Planning",id:"task-planning",level:3},{value:"Reactive vs Deliberative Behaviors",id:"reactive-vs-deliberative-behaviors",level:2},{value:"Reactive Behaviors",id:"reactive-behaviors",level:3},{value:"Deliberative Behaviors",id:"deliberative-behaviors",level:3},{value:"Learned Policies vs Rule-Based Logic",id:"learned-policies-vs-rule-based-logic",level:2},{value:"Learned Policies",id:"learned-policies",level:3},{value:"Rule-Based Logic",id:"rule-based-logic",level:3},{value:"Hybrid Approaches",id:"hybrid-approaches",level:3},{value:"Failure Handling and Replanning",id:"failure-handling-and-replanning",level:2},{value:"Failure Detection",id:"failure-detection",level:3},{value:"Recovery Strategies",id:"recovery-strategies",level:3},{value:"Planning Hierarchies",id:"planning-hierarchies",level:2},{value:"Strategic Planning",id:"strategic-planning",level:3},{value:"Tactical Planning",id:"tactical-planning",level:3},{value:"Operational Planning",id:"operational-planning",level:3},{value:"Decision-Making Frameworks",id:"decision-making-frameworks",level:2},{value:"Utility-Based Decision Making",id:"utility-based-decision-making",level:3},{value:"Behavior-Based Systems",id:"behavior-based-systems",level:3},{value:"Real-World Humanoid Examples",id:"real-world-humanoid-examples",level:2},{value:"Navigation Tasks",id:"navigation-tasks",level:3},{value:"Manipulation Tasks",id:"manipulation-tasks",level:3},{value:"Social Interaction",id:"social-interaction",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-3-planning--decision-making",children:"Chapter 3: Planning & Decision Making"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Planning and decision-making represent the cognitive layer of the AI-robot brain. While perception systems understand the current state of the world, planning and decision-making systems determine what actions to take to achieve goals. This chapter explores how robots plan their behavior, make decisions under uncertainty, and handle the complexities of real-world environments."}),"\n",(0,s.jsx)(n.p,{children:"The planning and decision-making layer bridges high-level goals with low-level control actions. It determines how to achieve objectives while respecting physical constraints, safety requirements, and environmental conditions. This layer must balance the need for intelligent, adaptive behavior with the requirements for reliable, safe operation."}),"\n",(0,s.jsx)(n.h2,{id:"types-of-planning",children:"Types of Planning"}),"\n",(0,s.jsx)(n.p,{children:"Robots must plan at multiple levels, from high-level task sequences to low-level motion trajectories. Different types of planning address different aspects of robot behavior."}),"\n",(0,s.jsx)(n.h3,{id:"motion-planning",children:"Motion Planning"}),"\n",(0,s.jsx)(n.p,{children:"Motion planning focuses on how to move the robot's body to achieve specific goals while avoiding obstacles and respecting physical constraints:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Path planning"}),": Computing collision-free paths through the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Trajectory planning"}),": Creating time-parameterized paths with velocity and acceleration profiles"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manipulation planning"}),": Planning arm movements for grasping and manipulation tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Whole-body planning"}),": Coordinating multiple parts of the robot simultaneously"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Motion planning algorithms must consider:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Collision avoidance"}),": Ensuring the robot doesn't collide with obstacles"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Kinematic constraints"}),": Respecting joint limits and reachability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamic constraints"}),": Considering balance and stability requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimization criteria"}),": Minimizing time, energy, or other objectives"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MotionPlanner:\n    """Plan robot motions for navigation and manipulation."""\n\n    def __init__(self):\n        self.environment_map = None\n        self.robot_model = None\n        self.planning_algorithms = {\n            \'path\': \'RRT*\',  # Rapidly-exploring Random Trees\n            \'trajectory\': \'polynomial\',  # Polynomial trajectory generation\n            \'optimization\': \'gradient_descent\'\n        }\n\n    def plan_navigation_path(self, start_pose, goal_pose):\n        """Plan collision-free path from start to goal."""\n        # Check if goal is reachable\n        if not self.is_reachable(goal_pose):\n            raise ValueError("Goal is not reachable")\n\n        # Plan path using selected algorithm\n        path = self.compute_path(start_pose, goal_pose)\n\n        # Optimize path for smoothness and efficiency\n        optimized_path = self.optimize_path(path)\n\n        return optimized_path\n\n    def plan_manipulation_trajectory(self, start_config, goal_config):\n        """Plan joint-space trajectory for manipulation."""\n        # Check joint limits\n        if not self.valid_configuration(goal_config):\n            raise ValueError("Goal configuration is invalid")\n\n        # Plan trajectory in joint space\n        trajectory = self.compute_joint_trajectory(start_config, goal_config)\n\n        # Verify trajectory is collision-free\n        collision_free = self.verify_trajectory_collisions(trajectory)\n\n        if not collision_free:\n            raise ValueError("Trajectory has collisions")\n\n        return trajectory\n\n    def compute_path(self, start_pose, goal_pose):\n        """Compute path using motion planning algorithm."""\n        # This would implement RRT*, A*, or similar algorithm\n        # Return sequence of poses from start to goal\n        pass\n\n    def optimize_path(self, path):\n        """Optimize path for smoothness and efficiency."""\n        # Smooth path to reduce sharp turns\n        # Shorten path while maintaining safety\n        pass\n\n    def compute_joint_trajectory(self, start_config, goal_config):\n        """Compute joint-space trajectory."""\n        # Generate smooth trajectory in joint space\n        # Consider velocity and acceleration limits\n        pass\n\n    def verify_trajectory_collisions(self, trajectory):\n        """Verify trajectory is collision-free."""\n        # Check each point in trajectory against environment\n        pass\n\n    def is_reachable(self, pose):\n        """Check if pose is reachable by robot."""\n        # Use inverse kinematics to check reachability\n        pass\n\n    def valid_configuration(self, config):\n        """Check if joint configuration is valid."""\n        # Check joint limits, self-collisions, etc.\n        pass\n'})}),"\n",(0,s.jsx)(n.h3,{id:"task-planning",children:"Task Planning"}),"\n",(0,s.jsx)(n.p,{children:"Task planning focuses on what sequence of actions to perform to achieve high-level goals:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal decomposition"}),": Breaking complex goals into simpler subtasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action sequencing"}),": Determining the order of actions to achieve goals"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource allocation"}),": Managing computational and physical resources"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Contingency planning"}),": Preparing alternative plans for different scenarios"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Task planning must consider:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Logical dependencies"}),": Some actions must precede others"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource constraints"}),": Limited computational and physical resources"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temporal constraints"}),": Deadlines and timing requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Uncertainty handling"}),": Dealing with unknown or stochastic outcomes"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class TaskPlanner:\n    """Plan high-level tasks and sequences of actions."""\n\n    def __init__(self):\n        self.knowledge_base = {}  # World knowledge and facts\n        self.goal_library = {}    # Predefined goals and methods\n        self.action_library = {}  # Available actions and effects\n\n    def plan_task_sequence(self, high_level_goal):\n        """Plan sequence of actions to achieve high-level goal."""\n        # Decompose goal into subgoals\n        subgoals = self.decompose_goal(high_level_goal)\n\n        # Sequence actions for each subgoal\n        action_sequence = self.sequence_actions(subgoals)\n\n        # Optimize sequence for efficiency\n        optimized_sequence = self.optimize_sequence(action_sequence)\n\n        return optimized_sequence\n\n    def decompose_goal(self, goal):\n        """Decompose high-level goal into subgoals."""\n        # Use predefined methods or learning-based decomposition\n        # Return sequence of subgoals\n        pass\n\n    def sequence_actions(self, subgoals):\n        """Sequence actions to achieve subgoals."""\n        # Determine order of actions considering dependencies\n        # Ensure logical and temporal constraints are satisfied\n        pass\n\n    def optimize_sequence(self, sequence):\n        """Optimize action sequence for efficiency."""\n        # Minimize resource usage, time, or other criteria\n        # Consider parallelizable actions\n        pass\n\n    def handle_uncertainty(self, plan):\n        """Add contingency plans for uncertainty."""\n        # Identify potential failure points\n        # Add monitoring and recovery actions\n        # Create alternative execution paths\n        pass\n\n    def update_knowledge_base(self, new_information):\n        """Update knowledge base with new information."""\n        # Incorporate new facts about world state\n        # Update beliefs about object locations, etc.\n        pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"reactive-vs-deliberative-behaviors",children:"Reactive vs Deliberative Behaviors"}),"\n",(0,s.jsx)(n.p,{children:"Robots must balance reactive responses to immediate stimuli with deliberate planning for long-term goals. Different types of behaviors serve different purposes in robot operation."}),"\n",(0,s.jsx)(n.h3,{id:"reactive-behaviors",children:"Reactive Behaviors"}),"\n",(0,s.jsx)(n.p,{children:"Reactive behaviors provide immediate responses to environmental stimuli without complex reasoning:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Characteristics"}),": Fast, predictable, reliable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Applications"}),": Obstacle avoidance, reflexive responses, emergency reactions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advantages"}),": Immediate response, guaranteed performance, simple implementation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disadvantages"}),": Limited flexibility, cannot handle complex situations"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Reactive behaviors are essential for safety and basic navigation, providing reliable responses to common situations."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ReactiveController:\n    """Handle reactive behaviors for immediate responses."""\n\n    def __init__(self):\n        self.safety_thresholds = {\n            \'proximity\': 0.5,  # meters\n            \'velocity\': 1.0,   # m/s\n            \'acceleration\': 2.0  # m/s\xb2\n        }\n\n    def handle_proximity_avoidance(self, sensor_data):\n        """Reactive obstacle avoidance."""\n        closest_obstacle = self.find_closest_obstacle(sensor_data)\n\n        if closest_obstacle < self.safety_thresholds[\'proximity\']:\n            # Immediate evasive action\n            evasive_command = self.generate_evasive_action(closest_obstacle)\n            return evasive_command\n\n        return None  # No action needed\n\n    def handle_balance_recovery(self, imu_data):\n        """Reactive balance recovery."""\n        tilt_angle = self.calculate_tilt_angle(imu_data)\n\n        if abs(tilt_angle) > self.safety_thresholds[\'tilt\']:\n            # Immediate balance recovery action\n            recovery_command = self.generate_balance_recovery(tilt_angle)\n            return recovery_command\n\n        return None  # Within safe limits\n\n    def find_closest_obstacle(self, sensor_data):\n        """Find closest obstacle from sensor data."""\n        # Process sensor data to find nearest obstacle\n        pass\n\n    def generate_evasive_action(self, obstacle_info):\n        """Generate evasive action based on obstacle."""\n        # Return immediate command to avoid obstacle\n        pass\n\n    def calculate_tilt_angle(self, imu_data):\n        """Calculate robot tilt angle from IMU."""\n        # Use IMU data to determine tilt\n        pass\n\n    def generate_balance_recovery(self, tilt_angle):\n        """Generate balance recovery action."""\n        # Return command to restore balance\n        pass\n'})}),"\n",(0,s.jsx)(n.h3,{id:"deliberative-behaviors",children:"Deliberative Behaviors"}),"\n",(0,s.jsx)(n.p,{children:"Deliberative behaviors involve complex reasoning and planning for long-term goals:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Characteristics"}),": Computationally intensive, flexible, goal-oriented"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Applications"}),": Complex task execution, strategic planning, problem solving"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advantages"}),": Handles complex situations, achieves long-term goals, adaptable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disadvantages"}),": Computationally expensive, slower response, may fail in emergencies"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Deliberative behaviors enable robots to perform complex tasks that require reasoning and planning."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class DeliberativePlanner:\n    """Handle deliberative planning and reasoning."""\n\n    def __init__(self):\n        self.reasoning_engine = None\n        self.long_term_memory = {}\n        self.goal_hierarchy = []\n\n    def execute_complex_task(self, complex_goal):\n        """Execute complex task requiring deliberative planning."""\n        # Analyze goal and decompose into subproblems\n        plan = self.analyze_and_plan(complex_goal)\n\n        # Execute plan with monitoring and adaptation\n        execution_result = self.execute_with_monitoring(plan)\n\n        return execution_result\n\n    def analyze_and_plan(self, goal):\n        """Analyze goal and create detailed plan."""\n        # Use knowledge base to understand goal requirements\n        # Consider multiple possible approaches\n        # Select optimal strategy based on criteria\n        pass\n\n    def execute_with_monitoring(self, plan):\n        """Execute plan with monitoring and adaptation."""\n        # Execute plan step by step\n        # Monitor progress and outcomes\n        # Adapt plan as needed based on feedback\n        pass\n\n    def reason_about_context(self, situation):\n        """Reason about current situation and context."""\n        # Use knowledge base to understand situation\n        # Consider relevant facts and constraints\n        # Generate appropriate responses\n        pass\n\n    def learn_from_execution(self, results):\n        """Learn from plan execution results."""\n        # Update knowledge base with lessons learned\n        # Improve future planning based on experience\n        pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"learned-policies-vs-rule-based-logic",children:"Learned Policies vs Rule-Based Logic"}),"\n",(0,s.jsx)(n.p,{children:"Modern AI-robot brains combine learned policies with rule-based logic to achieve both adaptability and reliability."}),"\n",(0,s.jsx)(n.h3,{id:"learned-policies",children:"Learned Policies"}),"\n",(0,s.jsx)(n.p,{children:"Learned policies use machine learning to determine appropriate actions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advantages"}),": Adaptive to complex situations, can learn from experience, handles uncertainty well"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Applications"}),": Motor skill learning, adaptive behavior, complex control tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Characteristics"}),": Data-driven, improves with experience, may be interpretable or opaque"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class LearnedPolicy:\n    """A learned policy for adaptive behavior."""\n\n    def __init__(self, policy_model):\n        self.model = policy_model\n        self.state_representation = None\n        self.action_space = None\n\n    def select_action(self, current_state):\n        """Select action based on learned policy."""\n        # Convert state to policy input format\n        policy_input = self.format_state_input(current_state)\n\n        # Get action from policy model\n        action = self.model.predict(policy_input)\n\n        # Convert action to robot command\n        robot_command = self.format_action_output(action)\n\n        return robot_command\n\n    def format_state_input(self, state):\n        """Format state for policy model input."""\n        # Extract relevant features from state\n        # Normalize or scale as needed\n        pass\n\n    def format_action_output(self, action):\n        """Format policy action for robot execution."""\n        # Convert policy action to robot command\n        # Apply safety limits and constraints\n        pass\n\n    def update_policy(self, experience_data):\n        """Update policy based on new experience."""\n        # Train model on new experience data\n        # Validate policy performance\n        pass\n'})}),"\n",(0,s.jsx)(n.h3,{id:"rule-based-logic",children:"Rule-Based Logic"}),"\n",(0,s.jsx)(n.p,{children:"Rule-based systems use explicit programming to determine actions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advantages"}),": Predictable, interpretable, reliable, safety-guaranteed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Applications"}),": Safety systems, critical operations, predictable scenarios"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Characteristics"}),": Explicit rules, deterministic behavior, human-readable logic"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class RuleBasedSystem:\n    \"\"\"A rule-based system for predictable behavior.\"\"\"\n\n    def __init__(self):\n        self.rule_base = [\n            {'condition': 'obstacle_close', 'action': 'stop_and_avoid'},\n            {'condition': 'battery_low', 'action': 'return_to_charger'},\n            {'condition': 'goal_reached', 'action': 'report_success'},\n            {'condition': 'emergency', 'action': 'activate_safety_protocol'}\n        ]\n\n    def select_action(self, current_state):\n        \"\"\"Select action based on rule evaluation.\"\"\"\n        for rule in self.rule_base:\n            if self.evaluate_condition(rule['condition'], current_state):\n                return self.execute_action(rule['action'], current_state)\n\n        # Default action if no rules match\n        return self.default_action(current_state)\n\n    def evaluate_condition(self, condition, state):\n        \"\"\"Evaluate rule condition against current state.\"\"\"\n        # Check if condition is met based on state\n        pass\n\n    def execute_action(self, action, state):\n        \"\"\"Execute rule-based action.\"\"\"\n        # Perform action specified by rule\n        pass\n\n    def default_action(self, state):\n        \"\"\"Return default action when no rules match.\"\"\"\n        # Safe default behavior\n        pass\n"})}),"\n",(0,s.jsx)(n.h3,{id:"hybrid-approaches",children:"Hybrid Approaches"}),"\n",(0,s.jsx)(n.p,{children:"Modern systems often combine both approaches:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),": Rule-based safety systems with learned policies for flexibility"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Benefits"}),": Safety guarantees with adaptive capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implementation"}),": Learned policies operate within rule-based safety constraints"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class HybridController:\n    """Combine learned policies with rule-based safety."""\n\n    def __init__(self):\n        self.learned_policy = None  # Learned policy model\n        self.rule_based_system = RuleBasedSystem()  # Safety rules\n        self.arbitration_logic = None  # Decision logic\n\n    def select_action(self, current_state):\n        """Select action using hybrid approach."""\n        # Get candidate action from learned policy\n        policy_action = self.learned_policy.select_action(current_state)\n\n        # Get safety action from rule-based system\n        safety_action = self.rule_based_system.select_action(current_state)\n\n        # Arbitrate between policy and safety actions\n        final_action = self.arbitrate_actions(policy_action, safety_action, current_state)\n\n        return final_action\n\n    def arbitrate_actions(self, policy_action, safety_action, state):\n        """Arbitrate between policy and safety actions."""\n        # Prioritize safety actions over policy actions\n        # Allow policy actions when safe\n        pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"failure-handling-and-replanning",children:"Failure Handling and Replanning"}),"\n",(0,s.jsx)(n.p,{children:"Robots must handle failures gracefully and adapt their plans when situations change unexpectedly."}),"\n",(0,s.jsx)(n.h3,{id:"failure-detection",children:"Failure Detection"}),"\n",(0,s.jsx)(n.p,{children:"Robots need to detect when plans are failing or when unexpected situations arise:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance monitoring"}),": Track plan execution progress"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Anomaly detection"}),": Identify deviations from expected behavior"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor validation"}),": Verify sensor data reliability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal assessment"}),": Determine if goals remain achievable"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"recovery-strategies",children:"Recovery Strategies"}),"\n",(0,s.jsx)(n.p,{children:"When failures occur, robots need appropriate recovery strategies:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback behaviors"}),": Predefined safe responses to common failures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Replanning"}),": Generate new plans when current plans fail"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal revision"}),": Modify goals when original goals become unachievable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human intervention"}),": Request help when autonomous recovery fails"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class FailureHandler:\n    """Handle failures and enable recovery."""\n\n    def __init__(self):\n        self.recovery_behaviors = {\n            \'navigation_failure\': \'return_to_known_location\',\n            \'manipulation_failure\': \'release_and_retry\',\n            \'communication_loss\': \'wait_and_retry\',\n            \'safety_violation\': \'emergency_stop\'\n        }\n        self.recovery_history = []\n\n    def detect_failure(self, execution_state):\n        """Detect when plan execution is failing."""\n        # Monitor execution progress\n        # Check for anomalies or deviations\n        # Assess goal achievability\n        pass\n\n    def initiate_recovery(self, failure_type, context):\n        """Initiate appropriate recovery behavior."""\n        if failure_type in self.recovery_behaviors:\n            recovery_action = self.recovery_behaviors[failure_type]\n            return self.execute_recovery(recovery_action, context)\n\n        # Default recovery if specific behavior not found\n        return self.default_recovery(context)\n\n    def execute_recovery(self, recovery_action, context):\n        """Execute specific recovery action."""\n        # Perform recovery action based on type\n        # Monitor recovery progress\n        # Update recovery history\n        pass\n\n    def replan_after_failure(self, original_goal, failure_context):\n        """Generate new plan after failure."""\n        # Analyze failure and context\n        # Modify original goal if necessary\n        # Generate new plan considering failure causes\n        pass\n\n    def default_recovery(self, context):\n        """Default recovery behavior."""\n        # Safe default action when specific recovery not known\n        pass\n\n    def learn_from_failures(self):\n        """Learn from failure and recovery experiences."""\n        # Update knowledge base with failure patterns\n        # Improve future failure handling\n        pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"planning-hierarchies",children:"Planning Hierarchies"}),"\n",(0,s.jsx)(n.p,{children:"Robots operate at multiple planning levels simultaneously:"}),"\n",(0,s.jsx)(n.h3,{id:"strategic-planning",children:"Strategic Planning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Time horizon"}),": Minutes to hours"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scope"}),": Mission-level objectives"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Frequency"}),": Occasional updates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Examples"}),": Daily schedules, long-term goals"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"tactical-planning",children:"Tactical Planning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Time horizon"}),": Seconds to minutes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scope"}),": Task-level activities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Frequency"}),": Periodic updates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Examples"}),": Task sequences, resource allocation"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"operational-planning",children:"Operational Planning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Time horizon"}),": Milliseconds to seconds"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scope"}),": Motion-level actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Frequency"}),": Continuous updates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Examples"}),": Path following, obstacle avoidance"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"decision-making-frameworks",children:"Decision-Making Frameworks"}),"\n",(0,s.jsx)(n.h3,{id:"utility-based-decision-making",children:"Utility-Based Decision Making"}),"\n",(0,s.jsx)(n.p,{children:"Robots can make decisions based on utility functions that quantify the desirability of different outcomes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Utility calculation"}),": Assign values to different possible outcomes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Uncertainty handling"}),": Account for probabilistic outcomes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-objective optimization"}),": Balance competing goals"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"behavior-based-systems",children:"Behavior-Based Systems"}),"\n",(0,s.jsx)(n.p,{children:"Behavior-based systems use collections of simple behaviors that compete for control:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Behavior arbitration"}),": Determine which behavior should be active"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Subsumption architecture"}),": Higher-level behaviors can inhibit lower-level ones"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Emergent behavior"}),": Complex behavior emerges from simple interactions"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"real-world-humanoid-examples",children:"Real-World Humanoid Examples"}),"\n",(0,s.jsx)(n.h3,{id:"navigation-tasks",children:"Navigation Tasks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Reach destination while avoiding obstacles"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Planning"}),": Path planning with dynamic obstacle avoidance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decision-making"}),": Balance efficiency vs safety"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"manipulation-tasks",children:"Manipulation Tasks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Grasp and manipulate objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Planning"}),": Reach planning, grasp planning, trajectory optimization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decision-making"}),": Choose grasp type, handle failures"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"social-interaction",children:"Social Interaction"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Engage in natural human-robot interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Planning"}),": Dialogue management, gesture planning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decision-making"}),": Choose appropriate responses"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter explored planning and decision-making systems that determine robot behavior. We examined the differences between motion and task planning, reactive and deliberative behaviors, learned policies and rule-based logic, and failure handling strategies."}),"\n",(0,s.jsx)(n.p,{children:"Planning and decision-making systems bridge the gap between high-level goals and low-level control, enabling robots to operate intelligently in complex environments. Modern systems often combine multiple approaches to achieve both adaptability and reliability."}),"\n",(0,s.jsx)(n.p,{children:"The key insight is that different types of planning and decision-making are needed for different aspects of robot behavior, and these systems must work together to create coherent, intelligent robot operation."}),"\n",(0,s.jsx)(n.p,{children:"In the next chapter, we'll explore how these planning and decision-making systems connect to ROS 2 for real robot control."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>o,x:()=>l});var a=i(6540);const s={},t=a.createContext(s);function o(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);