"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9],{8453(e,n,a){a.d(n,{R:()=>t,x:()=>o});var i=a(6540);const s={},r=i.createContext(s);function t(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(r.Provider,{value:n},e.children)}},9051(e,n,a){a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var i=a(4848),s=a(8453);const r={title:"Chapter 2: Sensor Simulation in Gazebo",description:"Simulating LiDAR, Depth Camera, and IMU sensors with ROS 2 integration",sidebar_position:6},t="Chapter 2: Sensor Simulation in Gazebo",o={id:"gazebo/chapters/chapter2",title:"Chapter 2: Sensor Simulation in Gazebo",description:"Simulating LiDAR, Depth Camera, and IMU sensors with ROS 2 integration",source:"@site/docs/gazebo/chapters/chapter2.md",sourceDirName:"gazebo/chapters",slug:"/gazebo/chapters/chapter2",permalink:"/physical-ai-humanoid-robotics-book/docs/gazebo/chapters/chapter2",draft:!1,unlisted:!1,editUrl:"https://github.com/ghulammustafa119/physical-ai-humanoid-robotics-book/tree/main/docs/gazebo/chapters/chapter2.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{title:"Chapter 2: Sensor Simulation in Gazebo",description:"Simulating LiDAR, Depth Camera, and IMU sensors with ROS 2 integration",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Chapter 1: Gazebo Overview and Physics Simulation",permalink:"/physical-ai-humanoid-robotics-book/docs/gazebo/chapters/chapter1"},next:{title:"Chapter 3: Unity for High-Fidelity Interaction",permalink:"/physical-ai-humanoid-robotics-book/docs/gazebo/chapters/chapter3"}},l={},c=[{value:"Introduction to Sensor Simulation",id:"introduction-to-sensor-simulation",level:2},{value:"LiDAR Sensor Simulation",id:"lidar-sensor-simulation",level:2},{value:"LiDAR Physics and Principles",id:"lidar-physics-and-principles",level:3},{value:"Realistic Parameter Configuration",id:"realistic-parameter-configuration",level:3},{value:"LiDAR Data Processing with ROS 2",id:"lidar-data-processing-with-ros-2",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"Depth Camera Physics and Optics",id:"depth-camera-physics-and-optics",level:3},{value:"Depth Data Processing",id:"depth-data-processing",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU Physics and Components",id:"imu-physics-and-components",level:3},{value:"IMU Data Processing",id:"imu-data-processing",level:3},{value:"ROS 2 Integration and Standard Messages",id:"ros-2-integration-and-standard-messages",level:2},{value:"Standard Message Types",id:"standard-message-types",level:3},{value:"Data Acquisition Patterns",id:"data-acquisition-patterns",level:3},{value:"Textual Visualization of Sensor Outputs",id:"textual-visualization-of-sensor-outputs",level:2},{value:"LiDAR Scan Visualization",id:"lidar-scan-visualization",level:3},{value:"Depth Camera Output Visualization",id:"depth-camera-output-visualization",level:3},{value:"IMU Data Representation",id:"imu-data-representation",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Sensor Update Rates",id:"sensor-update-rates",level:3},{value:"Computational Requirements",id:"computational-requirements",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"chapter-2-sensor-simulation-in-gazebo",children:"Chapter 2: Sensor Simulation in Gazebo"}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-sensor-simulation",children:"Introduction to Sensor Simulation"}),"\n",(0,i.jsx)(n.p,{children:"Sensor simulation is a critical component of effective robotics simulation, enabling the development and testing of perception algorithms without requiring physical hardware. In Gazebo, realistic sensor simulation bridges the gap between virtual and real-world robotics by generating data that closely matches the characteristics and noise patterns of actual sensors."}),"\n",(0,i.jsx)(n.p,{children:"For humanoid robots, accurate sensor simulation is essential for developing robust perception, navigation, and interaction capabilities. The three primary sensor types we'll focus on are:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiDAR sensors"}),": Provide 2D or 3D distance measurements for obstacle detection and mapping"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth cameras"}),": Offer color and depth information for 3D scene understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMUs"}),": Deliver orientation and motion data for state estimation"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"lidar-sensor-simulation",children:"LiDAR Sensor Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"lidar-physics-and-principles",children:"LiDAR Physics and Principles"}),"\n",(0,i.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) sensors work by emitting laser pulses and measuring the time it takes for the light to return after reflecting off objects. In Gazebo, LiDAR simulation uses ray tracing to detect obstacles and calculate distances:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- LiDAR sensor configuration --\x3e\n<sensor name="lidar_sensor" type="ray">\n  <pose>0.2 0 0.1 0 0 0</pose>  \x3c!-- Position on robot --\x3e\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>        \x3c!-- Number of rays per revolution --\x3e\n        <resolution>1</resolution>     \x3c!-- Angular resolution --\x3e\n        <min_angle>-3.14159</min_angle> \x3c!-- -\u03c0 radians (-180\xb0) --\x3e\n        <max_angle>3.14159</max_angle>  \x3c!-- \u03c0 radians (180\xb0) --\x3e\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>     \x3c!-- Minimum detection range (m) --\x3e\n      <max>30.0</max>    \x3c!-- Maximum detection range (m) --\x3e\n      <resolution>0.01</resolution>  \x3c!-- Range resolution (m) --\x3e\n    </range>\n  </ray>\n  <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n    <ros>\n      <namespace>/lidar</namespace>\n      <remapping>~/out:=scan</remapping>\n    </ros>\n    <output_type>sensor_msgs/LaserScan</output_type>\n    <frame_name>lidar_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"realistic-parameter-configuration",children:"Realistic Parameter Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Configuring realistic LiDAR parameters is essential for effective simulation:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Range"}),": Minimum and maximum detection distances should match the physical sensor"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resolution"}),": Angular and distance resolution should reflect the actual sensor capabilities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Noise"}),": Add realistic noise models to simulate real-world sensor imperfections"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Update rate"}),": Should match the physical sensor's update frequency"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Enhanced LiDAR with noise model --\x3e\n<sensor name="lidar_sensor" type="ray">\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>1080</samples>    \x3c!-- Higher resolution --\x3e\n        <resolution>1</resolution>\n        <min_angle>-2.35619</min_angle> \x3c!-- -135\xb0 --\x3e\n        <max_angle>2.35619</max_angle>   \x3c!-- 135\xb0 --\x3e\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.08</min>     \x3c!-- Hokuyo UTM-30LX like --\x3e\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <always_on>true</always_on>\n  <update_rate>40</update_rate>  \x3c!-- 40 Hz update rate --\x3e\n  <visualize>true</visualize>    \x3c!-- Visualize rays in GUI --\x3e\n\n  \x3c!-- Noise model --\x3e\n  <noise type="gaussian">\n    <mean>0.0</mean>\n    <stddev>0.01</stddev>  \x3c!-- 1cm standard deviation --\x3e\n  </noise>\n\n  <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n    <ros>\n      <namespace>/robot/lidar</namespace>\n      <remapping>~/out:=scan</remapping>\n    </ros>\n    <output_type>sensor_msgs/LaserScan</output_type>\n    <frame_name>lidar_link</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"lidar-data-processing-with-ros-2",children:"LiDAR Data Processing with ROS 2"}),"\n",(0,i.jsx)(n.p,{children:"Once the LiDAR data is published to ROS 2 topics, it can be processed using standard message types:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nimport numpy as np\n\nclass LidarProcessor(Node):\n    def __init__(self):\n        super().__init__('lidar_processor')\n\n        # Subscribe to LiDAR scan data\n        self.subscription = self.create_subscription(\n            LaserScan,\n            '/robot/lidar/scan',\n            self.lidar_callback,\n            10\n        )\n\n        # Publisher for processed data\n        self.obstacle_publisher = self.create_publisher(\n            # Custom message type for obstacle detection\n            # This would be defined in your package\n        )\n\n        self.get_logger().info('LiDAR processor initialized')\n\n    def lidar_callback(self, msg):\n        \"\"\"Process incoming LiDAR scan data\"\"\"\n        # Extract ranges from message\n        ranges = np.array(msg.ranges)\n\n        # Filter out invalid ranges (inf, nan)\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        # Calculate minimum distance (closest obstacle)\n        if len(valid_ranges) > 0:\n            min_distance = np.min(valid_ranges)\n            self.get_logger().info(f'Closest obstacle: {min_distance:.2f}m')\n\n        # Detect obstacles within threshold\n        obstacle_threshold = 1.0  # meters\n        obstacles = ranges[ranges < obstacle_threshold]\n\n        if len(obstacles) > 0:\n            self.get_logger().warn(f'{len(obstacles)} obstacles detected within {obstacle_threshold}m')\n\n        # Calculate free space statistics\n        free_space_distances = ranges[ranges > obstacle_threshold]\n        if len(free_space_distances) > 0:\n            avg_free_space = np.mean(free_space_distances)\n            self.get_logger().info(f'Average free space: {avg_free_space:.2f}m')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    lidar_processor = LidarProcessor()\n\n    try:\n        rclpy.spin(lidar_processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        lidar_processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"depth-camera-physics-and-optics",children:"Depth Camera Physics and Optics"}),"\n",(0,i.jsx)(n.p,{children:"Depth cameras capture both color and depth information, providing 3D spatial data essential for navigation and manipulation. In Gazebo, depth cameras use ray tracing to calculate depth values:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Depth camera sensor configuration --\x3e\n<sensor name="depth_camera" type="depth">\n  <pose>0.1 0 0.2 0 0 0</pose>  \x3c!-- Position on robot --\x3e\n  <camera name="depth_cam">\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees in radians --\x3e\n    <image>\n      <width>640</width>      \x3c!-- Image width --\x3e\n      <height>480</height>    \x3c!-- Image height --\x3e\n      <format>R8G8B8</format> \x3c!-- Color format --\x3e\n    </image>\n    <clip>\n      <near>0.1</near>    \x3c!-- Near clipping distance --\x3e\n      <far>10.0</far>     \x3c!-- Far clipping distance --\x3e\n    </clip>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.007</stddev>  \x3c!-- Noise standard deviation --\x3e\n    </noise>\n  </camera>\n  <always_on>true</always_on>\n  <update_rate>30</update_rate>  \x3c!-- 30 FPS --\x3e\n  <visualize>true</visualize>\n\n  <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n    <ros>\n      <namespace>/camera</namespace>\n      <remapping>image_raw:=image_color</remapping>\n      <remapping>camera_info:=camera_info</remapping>\n    </ros>\n    <frame_name>camera_link</frame_name>\n    <min_depth>0.1</min_depth>\n    <max_depth>10.0</max_depth>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"depth-data-processing",children:"Depth Data Processing"}),"\n",(0,i.jsx)(n.p,{children:"Depth camera data provides both color images and depth information that can be processed for 3D understanding:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass DepthCameraProcessor(Node):\n    def __init__(self):\n        super().__init__('depth_camera_processor')\n\n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n\n        # Subscribe to color and depth images\n        self.color_sub = self.create_subscription(\n            Image,\n            '/camera/image_color',\n            self.color_callback,\n            10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/camera/depth/image_raw',\n            self.depth_callback,\n            10\n        )\n\n        # Subscribe to camera info for intrinsic parameters\n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.info_callback,\n            10\n        )\n\n        self.camera_info = None\n        self.get_logger().info('Depth camera processor initialized')\n\n    def info_callback(self, msg):\n        \"\"\"Store camera intrinsic parameters\"\"\"\n        self.camera_info = msg\n\n    def color_callback(self, msg):\n        \"\"\"Process color image data\"\"\"\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Process the color image (example: edge detection)\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n            edges = cv2.Canny(gray, 50, 150)\n\n            # Display the result\n            cv2.imshow('Color Image', cv_image)\n            cv2.imshow('Edges', edges)\n            cv2.waitKey(1)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing color image: {e}')\n\n    def depth_callback(self, msg):\n        \"\"\"Process depth image data\"\"\"\n        try:\n            # Convert depth image to numpy array\n            depth_image = self.bridge.imgmsg_to_cv2(msg, \"32FC1\")\n\n            # Process depth data\n            height, width = depth_image.shape\n\n            # Calculate distance statistics\n            valid_depths = depth_image[np.isfinite(depth_image)]\n            if len(valid_depths) > 0:\n                min_depth = np.min(valid_depths)\n                max_depth = np.max(valid_depths)\n                avg_depth = np.mean(valid_depths)\n\n                self.get_logger().info(\n                    f'Depth stats - Min: {min_depth:.2f}m, '\n                    f'Max: {max_depth:.2f}m, Avg: {avg_depth:.2f}m'\n                )\n\n            # Find objects at specific distance range\n            distance_threshold = 2.0  # meters\n            close_objects = depth_image[depth_image < distance_threshold]\n\n            if len(close_objects) > 0:\n                percentage = (len(close_objects) / (height * width)) * 100\n                self.get_logger().info(\n                    f'{len(close_objects)} pixels within {distance_threshold}m '\n                    f'({percentage:.1f}% of image)'\n                )\n\n            # Display depth image (normalized for visualization)\n            depth_normalized = cv2.normalize(depth_image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC1)\n            cv2.imshow('Depth Image', depth_normalized)\n            cv2.waitKey(1)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing depth image: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    depth_processor = DepthCameraProcessor()\n\n    try:\n        rclpy.spin(depth_processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        cv2.destroyAllWindows()\n        depth_processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,i.jsx)(n.h3,{id:"imu-physics-and-components",children:"IMU Physics and Components"}),"\n",(0,i.jsx)(n.p,{children:"An IMU (Inertial Measurement Unit) combines accelerometers and gyroscopes to measure linear acceleration and angular velocity. In Gazebo, IMU simulation models the physics of these sensors:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- IMU sensor configuration --\x3e\n<sensor name="imu_sensor" type="imu">\n  <always_on>true</always_on>\n  <update_rate>100</update_rate>  \x3c!-- 100 Hz update rate --\x3e\n  <pose>0 0 0.1 0 0 0</pose>  \x3c!-- Position on robot --\x3e\n\n  <imu>\n    \x3c!-- Noise parameters for accelerometer --\x3e\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>  \x3c!-- ~0.1 deg/s --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n\n    \x3c!-- Noise parameters for gyroscope --\x3e\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>  \x3c!-- ~0.017 m/s^2 --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n\n  <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">\n    <ros>\n      <namespace>/imu</namespace>\n      <remapping>~/out:=data</remapping>\n    </ros>\n    <frame_name>imu_link</frame_name>\n    <initial_orientation_as_reference>false</initial_orientation_as_reference>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"imu-data-processing",children:"IMU Data Processing"}),"\n",(0,i.jsx)(n.p,{children:"IMU data provides crucial information about the robot's orientation and motion:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass IMUProcessor(Node):\n    def __init__(self):\n        super().__init__('imu_processor')\n\n        # Subscribe to IMU data\n        self.subscription = self.create_subscription(\n            Imu,\n            '/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        # Publisher for processed orientation data\n        self.orientation_publisher = self.create_publisher(\n            Vector3,\n            '/robot/orientation_euler',\n            10\n        )\n\n        self.previous_orientation = None\n        self.get_logger().info('IMU processor initialized')\n\n    def quaternion_to_euler(self, x, y, z, w):\n        \"\"\"Convert quaternion to Euler angles (roll, pitch, yaw)\"\"\"\n        # Create rotation object from quaternion\n        r = R.from_quat([x, y, z, w])\n        # Convert to Euler angles in radians\n        euler = r.as_euler('xyz')\n        return euler\n\n    def imu_callback(self, msg):\n        \"\"\"Process incoming IMU data\"\"\"\n        # Extract orientation quaternion\n        orientation = msg.orientation\n        euler = self.quaternion_to_euler(\n            orientation.x, orientation.y, orientation.z, orientation.w\n        )\n\n        # Convert to degrees for easier interpretation\n        roll_deg = np.degrees(euler[0])\n        pitch_deg = np.degrees(euler[1])\n        yaw_deg = np.degrees(euler[2])\n\n        # Log orientation data\n        self.get_logger().info(\n            f'Orientation - Roll: {roll_deg:.2f}\xb0, '\n            f'Pitch: {pitch_deg:.2f}\xb0, Yaw: {yaw_deg:.2f}\xb0'\n        )\n\n        # Extract angular velocity\n        angular_vel = msg.angular_velocity\n        self.get_logger().info(\n            f'Angular Vel - X: {angular_vel.x:.3f}, '\n            f'Y: {angular_vel.y:.3f}, Z: {angular_vel.z:.3f}'\n        )\n\n        # Extract linear acceleration\n        linear_acc = msg.linear_acceleration\n        total_acc = np.sqrt(\n            linear_acc.x**2 + linear_acc.y**2 + linear_acc.z**2\n        )\n        self.get_logger().info(\n            f'Linear Acc - X: {linear_acc.x:.3f}, '\n            f'Y: {linear_acc.y:.3f}, Z: {linear_acc.z:.3f}, '\n            f'Total: {total_acc:.3f}'\n        )\n\n        # Publish Euler angles for other nodes to use\n        euler_msg = Vector3()\n        euler_msg.x = roll_deg\n        euler_msg.y = pitch_deg\n        euler_msg.z = yaw_deg\n        self.orientation_publisher.publish(euler_msg)\n\n        # Detect significant orientation changes\n        if self.previous_orientation is not None:\n            prev_euler = self.previous_orientation\n            change = abs(euler - prev_euler)\n            max_change = np.max(change)\n\n            if max_change > np.radians(5):  # 5 degree threshold\n                self.get_logger().warn(\n                    f'Significant orientation change detected: {np.degrees(max_change):.2f}\xb0'\n                )\n\n        self.previous_orientation = euler\n\ndef main(args=None):\n    rclpy.init(args=args)\n    imu_processor = IMUProcessor()\n\n    try:\n        rclpy.spin(imu_processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        imu_processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"ros-2-integration-and-standard-messages",children:"ROS 2 Integration and Standard Messages"}),"\n",(0,i.jsx)(n.h3,{id:"standard-message-types",children:"Standard Message Types"}),"\n",(0,i.jsx)(n.p,{children:"Gazebo maps sensor data to standard ROS 2 message types that ensure compatibility across the robotics ecosystem:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"sensor_msgs/LaserScan"}),": For LiDAR data with ranges, intensities, and metadata"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"sensor_msgs/Image"}),": For camera images with encoding and size information"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"sensor_msgs/Imu"}),": For IMU data with orientation, angular velocity, and linear acceleration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"sensor_msgs/CameraInfo"}),": For camera intrinsic and extrinsic parameters"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data-acquisition-patterns",children:"Data Acquisition Patterns"}),"\n",(0,i.jsx)(n.p,{children:"The typical pattern for sensor data acquisition in ROS 2 involves:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor publishing"}),": Gazebo plugins publish sensor data to ROS 2 topics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Message subscription"}),": Robot nodes subscribe to sensor topics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data processing"}),": Nodes process sensor data for perception or control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Result publication"}),": Processed data is published for other nodes to use"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu, Image\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\n\nclass MultiSensorFusion(Node):\n    def __init__(self):\n        super().__init__('multi_sensor_fusion')\n\n        # Create subscribers for multiple sensors\n        lidar_sub = Subscriber(self, LaserScan, '/robot/lidar/scan')\n        imu_sub = Subscriber(self, Imu, '/imu/data')\n        camera_sub = Subscriber(self, Image, '/camera/image_color')\n\n        # Synchronize messages from different sensors\n        ats = ApproximateTimeSynchronizer(\n            [lidar_sub, imu_sub, camera_sub],\n            queue_size=10,\n            slop=0.1  # 100ms tolerance\n        )\n        ats.registerCallback(self.sensor_fusion_callback)\n\n        self.get_logger().info('Multi-sensor fusion node initialized')\n\n    def sensor_fusion_callback(self, lidar_msg, imu_msg, camera_msg):\n        \"\"\"Process synchronized sensor data\"\"\"\n        self.get_logger().info('Received synchronized sensor data')\n\n        # Perform sensor fusion operations here\n        # Example: combine LiDAR obstacle detection with IMU orientation\n        # to determine robot's pose in the environment\n\n        # Extract relevant information from each sensor\n        lidar_ranges = lidar_msg.ranges\n        imu_orientation = imu_msg.orientation\n        camera_encoding = camera_msg.encoding\n\n        # Log fusion result\n        self.get_logger().info(\n            f'Fusion: {len(lidar_ranges)} LiDAR points, '\n            f'Camera encoding: {camera_encoding}'\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    fusion_node = MultiSensorFusion()\n\n    try:\n        rclpy.spin(fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"textual-visualization-of-sensor-outputs",children:"Textual Visualization of Sensor Outputs"}),"\n",(0,i.jsx)(n.h3,{id:"lidar-scan-visualization",children:"LiDAR Scan Visualization"}),"\n",(0,i.jsx)(n.p,{children:"LiDAR sensors provide 2D or 3D distance measurements that can be visualized as point clouds or occupancy grids:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"LiDAR Scan Visualization (Top-down view):\n\n     3m\n      \u2191\n-3m \u2190 \u2192 +3m\n      \u2193\n    -3m\n\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n    \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7 \xb7\n\nLegend:\n\xb7 = Free space\n\u2588 = Obstacle detected\nO = Robot position\n"})}),"\n",(0,i.jsx)(n.h3,{id:"depth-camera-output-visualization",children:"Depth Camera Output Visualization"}),"\n",(0,i.jsx)(n.p,{children:"Depth cameras provide both color and depth information that can be processed for 3D understanding:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Depth Camera Output Structure:\n\nColor Image (RGB):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 R G B R G B ... \u2502 \u2190 Each pixel has RGB values\n\u2502 R G B R G B ... \u2502\n\u2502 R G B R G B ... \u2502\n\u2502 ...     ...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nDepth Image (Float32):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1.2 1.5 2.1 ... \u2502 \u2190 Each pixel has depth in meters\n\u2502 1.3 1.4 2.0 ... \u2502\n\u2502 1.4 1.3 1.9 ... \u2502\n\u2502 ...     ...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nPoint Cloud (X,Y,Z):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502(1,2,1.2)(1,3,1.5)\u2502 \u2190 Each pixel becomes 3D point\n\u2502(2,2,1.4)(2,3,1.3)\u2502\n\u2502(3,2,1.5)(3,3,1.2)\u2502\n\u2502 ...      ...     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"imu-data-representation",children:"IMU Data Representation"}),"\n",(0,i.jsx)(n.p,{children:"IMU sensors provide orientation, angular velocity, and linear acceleration data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"IMU Data Structure:\n\nOrientation (Quaternion):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x: 0.0, y: 0.0,       \u2502\n\u2502 z: 0.7, w: 0.7        \u2502 \u2190 45\xb0 rotation around Z-axis\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAngular Velocity:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x: 0.1, y: 0.0,       \u2502 \u2190 0.1 rad/s rotation around X\n\u2502 z: 0.0                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nLinear Acceleration:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x: 0.0, y: 0.0,       \u2502 \u2190 9.8 m/s\xb2 due to gravity\n\u2502 z: 9.8                \u2502   when Z-axis is up\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"sensor-update-rates",children:"Sensor Update Rates"}),"\n",(0,i.jsx)(n.p,{children:"Different sensors have different optimal update rates based on their purpose:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiDAR"}),": 10-40 Hz for navigation and mapping"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth Camera"}),": 15-30 Hz for 3D perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU"}),": 100-200 Hz for accurate motion tracking"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"computational-requirements",children:"Computational Requirements"}),"\n",(0,i.jsx)(n.p,{children:"Sensor simulation places different computational loads on the system:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiDAR"}),": Moderate CPU usage, ray tracing calculations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth Camera"}),": High GPU usage, rendering calculations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU"}),": Low computational requirements, physics integration"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter has covered the essential sensor types for humanoid robot simulation in Gazebo: LiDAR, depth cameras, and IMUs. Each sensor type provides different information that is crucial for robot perception and navigation:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiDAR sensors"})," provide 2D or 3D distance measurements for obstacle detection and mapping"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth cameras"})," offer color and depth information for 3D scene understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMUs"})," deliver orientation and motion data for state estimation"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The chapter demonstrated how to configure these sensors in Gazebo with realistic parameters, process their data using ROS 2 standard message types, and visualize the results for debugging and validation. Proper sensor simulation is fundamental to developing robust robotics applications that can successfully transition from simulation to reality."}),"\n",(0,i.jsx)(n.p,{children:"In the next chapter, we'll explore Unity as a platform for high-fidelity visualization and human-robot interaction simulation."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);