"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[713],{8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}},9766(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var s=t(4848),i=t(8453);const o={title:"Chapter 2: Vision-Language Models (VLMs)",description:"Transformer-based architectures and attention mechanisms for robotic perception",sidebar_position:14},a="Chapter 2: Vision-Language Models (VLMs)",r={id:"vla/chapters/chapter2",title:"Chapter 2: Vision-Language Models (VLMs)",description:"Transformer-based architectures and attention mechanisms for robotic perception",source:"@site/docs/vla/chapters/chapter2.md",sourceDirName:"vla/chapters",slug:"/vla/chapters/chapter2",permalink:"/physical-ai-humanoid-robotics-book/docs/vla/chapters/chapter2",draft:!1,unlisted:!1,editUrl:"https://github.com/ghulammustafa119/physical-ai-humanoid-robotics-book/tree/main/docs/vla/chapters/chapter2.md",tags:[],version:"current",sidebarPosition:14,frontMatter:{title:"Chapter 2: Vision-Language Models (VLMs)",description:"Transformer-based architectures and attention mechanisms for robotic perception",sidebar_position:14},sidebar:"tutorialSidebar",previous:{title:"Chapter 1: Introduction to VLA",permalink:"/physical-ai-humanoid-robotics-book/docs/vla/chapters/chapter1"},next:{title:"Chapter 3: Action Planning & Execution",permalink:"/physical-ai-humanoid-robotics-book/docs/vla/chapters/chapter3"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Transformer-Based Architectures",id:"transformer-based-architectures",level:2},{value:"Introduction to Transformers in VLMs",id:"introduction-to-transformers-in-vlms",level:3},{value:"Vision Transformer (ViT) Architecture",id:"vision-transformer-vit-architecture",level:3},{value:"Text Transformer Architecture",id:"text-transformer-architecture",level:3},{value:"Vision-Language Fusion Architectures",id:"vision-language-fusion-architectures",level:3},{value:"Early Fusion",id:"early-fusion",level:4},{value:"Late Fusion",id:"late-fusion",level:4},{value:"Cross-Attention Fusion",id:"cross-attention-fusion",level:4},{value:"Attention Mechanisms",id:"attention-mechanisms",level:2},{value:"Self-Attention in VLMs",id:"self-attention-in-vlms",level:3},{value:"Cross-Modal Attention",id:"cross-modal-attention",level:3},{value:"Multi-Head Attention",id:"multi-head-attention",level:3},{value:"Perception Pipelines in Robotics",id:"perception-pipelines-in-robotics",level:2},{value:"Visual Feature Extraction",id:"visual-feature-extraction",level:3},{value:"Language Processing for Robotics",id:"language-processing-for-robotics",level:3},{value:"Multimodal Fusion for Action Grounding",id:"multimodal-fusion-for-action-grounding",level:3},{value:"Implementation Considerations for Robotics",id:"implementation-considerations-for-robotics",level:2},{value:"Real-Time Processing Requirements",id:"real-time-processing-requirements",level:3},{value:"Memory and Computation Constraints",id:"memory-and-computation-constraints",level:3},{value:"Robustness and Safety",id:"robustness-and-safety",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Message Types for VLM Outputs",id:"message-types-for-vlm-outputs",level:3},{value:"Publisher Implementation",id:"publisher-implementation",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"chapter-2-vision-language-models-vlms",children:"Chapter 2: Vision-Language Models (VLMs)"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Vision-Language Models (VLMs) form the backbone of modern Vision-Language-Action (VLA) systems, providing the essential capability to connect visual perception with natural language understanding. These models enable humanoid robots to interpret complex visual scenes and understand human instructions expressed in natural language, bridging the gap between symbolic linguistic concepts and grounded visual representations."}),"\n",(0,s.jsx)(n.p,{children:'VLMs leverage transformer architectures and attention mechanisms to create unified representations that span visual and linguistic modalities. This integration allows robots to understand that the word "cup" corresponds to specific visual patterns, that "red" describes a visual attribute, and that spatial relationships like "on the table" connect objects in both visual and linguistic spaces.'}),"\n",(0,s.jsx)(n.h2,{id:"transformer-based-architectures",children:"Transformer-Based Architectures"}),"\n",(0,s.jsx)(n.h3,{id:"introduction-to-transformers-in-vlms",children:"Introduction to Transformers in VLMs"}),"\n",(0,s.jsx)(n.p,{children:"Transformers have revolutionized multimodal understanding by providing a unified architecture capable of processing sequences of different modalities. In the context of VLA systems, transformers enable the joint processing of visual features (often converted to patch sequences) and textual tokens, creating shared representations that capture relationships between vision and language."}),"\n",(0,s.jsx)(n.p,{children:"The key innovation of transformers lies in their self-attention mechanism, which allows each element in a sequence to attend to all other elements, regardless of their position. This enables the model to capture long-range dependencies and complex relationships between visual and linguistic elements."}),"\n",(0,s.jsx)(n.h3,{id:"vision-transformer-vit-architecture",children:"Vision Transformer (ViT) Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Vision Transformers adapt the original transformer architecture for visual processing by treating images as sequences of patches. The process involves:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Patching"}),": The input image is divided into fixed-size patches (e.g., 16x16 pixels)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Linear Embedding"}),": Each patch is flattened and projected to a fixed-dimensional vector"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Positional Encoding"}),": Positional information is added to maintain spatial relationships"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transformer Processing"}),": The sequence of patch embeddings is processed by transformer layers"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim):\n        super().__init__()\n        assert image_size % patch_size == 0, "Image dimensions must be divisible by patch size"\n\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = 3 * patch_size ** 2  # RGB channels\n\n        self.patch_size = patch_size\n        self.to_patch_embedding = nn.Sequential(\n            nn.Linear(patch_dim, dim),\n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))  # +1 for CLS token\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim),\n            num_layers=depth\n        )\n\n        self.to_latent = nn.Identity()\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, img):\n        batch_size, channels, height, width = img.shape\n        assert height == width, "Input image must be square"\n\n        # Convert image to patches\n        patch_size = self.patch_size\n        num_patches_h = height // patch_size\n        num_patches_w = width // patch_size\n\n        patches = img.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n        patches = patches.contiguous().view(batch_size, channels, num_patches_h * num_patches_w, patch_size, patch_size)\n        patches = patches.transpose(1, 2).contiguous().view(batch_size, num_patches_h * num_patches_w, -1)\n\n        # Linear embedding\n        x = self.to_patch_embedding(patches)\n\n        # Add CLS token\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # Add positional embedding\n        x += self.pos_embedding[:, :(num_patches_h * num_patches_w + 1)]\n\n        # Transformer processing\n        x = self.transformer(x)\n\n        # Use CLS token for classification\n        x = x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"text-transformer-architecture",children:"Text Transformer Architecture"}),"\n",(0,s.jsx)(n.p,{children:"For language processing, transformers use tokenization to convert text into sequences of token IDs, which are then embedded and processed through transformer layers. The text transformer handles:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tokenization"}),": Converting text to discrete tokens"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedding"}),": Mapping tokens to continuous vector representations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Positional Encoding"}),": Adding information about token positions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transformer Processing"}),": Applying self-attention and feed-forward layers"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"vision-language-fusion-architectures",children:"Vision-Language Fusion Architectures"}),"\n",(0,s.jsx)(n.p,{children:"VLMs combine vision and language processing through various fusion strategies:"}),"\n",(0,s.jsx)(n.h4,{id:"early-fusion",children:"Early Fusion"}),"\n",(0,s.jsx)(n.p,{children:"In early fusion approaches, visual and textual features are concatenated early in the processing pipeline and jointly processed through shared transformer layers. This allows for tight integration but can be computationally expensive."}),"\n",(0,s.jsx)(n.h4,{id:"late-fusion",children:"Late Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Late fusion processes visual and textual inputs separately before combining them at later stages. This approach is more modular but may miss fine-grained cross-modal interactions."}),"\n",(0,s.jsx)(n.h4,{id:"cross-attention-fusion",children:"Cross-Attention Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Cross-attention mechanisms allow each modality to attend to the other, enabling rich bidirectional interactions. In VLA systems, this might involve visual features attending to relevant text tokens and vice versa."}),"\n",(0,s.jsx)(n.h2,{id:"attention-mechanisms",children:"Attention Mechanisms"}),"\n",(0,s.jsx)(n.h3,{id:"self-attention-in-vlms",children:"Self-Attention in VLMs"}),"\n",(0,s.jsx)(n.p,{children:"The attention mechanism in transformers computes a weighted sum of value vectors, where the weights are determined by compatibility between query and key vectors. In mathematical terms:"}),"\n",(0,s.jsx)(n.p,{children:"Attention(Q, K, V) = softmax(QK^T / \u221ad_k)V"}),"\n",(0,s.jsx)(n.p,{children:"Where Q, K, and V are query, key, and value matrices derived from input embeddings."}),"\n",(0,s.jsx)(n.p,{children:'In VLMs, self-attention operates across both visual and linguistic elements, allowing the model to discover relationships between image patches and text tokens. For example, when processing "the red ball on the table," attention weights might highlight the image patches corresponding to the red ball when processing the tokens "red" and "ball."'}),"\n",(0,s.jsx)(n.h3,{id:"cross-modal-attention",children:"Cross-Modal Attention"}),"\n",(0,s.jsx)(n.p,{children:"Cross-modal attention enables information flow between different modalities. In vision-language contexts:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Visual-to-Text Attention: Visual features attend to relevant text tokens"}),"\n",(0,s.jsx)(n.li,{children:"Text-to-Visual Attention: Text features attend to relevant visual regions"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This mechanism is crucial for grounding language understanding in visual perception and vice versa."}),"\n",(0,s.jsx)(n.h3,{id:"multi-head-attention",children:"Multi-Head Attention"}),"\n",(0,s.jsx)(n.p,{children:"Multi-head attention allows the model to focus on different aspects of the input simultaneously by using multiple attention heads with different learned projections:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        matmul_qk = torch.matmul(Q, K.transpose(-2, -1))\n        scaled_attention_logits = matmul_qk / math.sqrt(self.d_k)\n\n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n\n        attention_weights = torch.softmax(scaled_attention_logits, dim=-1)\n        output = torch.matmul(attention_weights, V)\n\n        return output, attention_weights\n\n    def split_heads(self, x):\n        batch_size = x.size(0)\n        seq_len = x.size(1)\n\n        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n        return x.transpose(1, 2)\n\n    def forward(self, Q, K, V, mask=None):\n        Q = self.W_q(Q)\n        K = self.W_k(K)\n        V = self.W_v(V)\n\n        Q = self.split_heads(Q)\n        K = self.split_heads(K)\n        V = self.split_heads(V)\n\n        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n            Q, K, V, mask)\n\n        scaled_attention = scaled_attention.transpose(1, 2).contiguous()\n        scaled_attention = scaled_attention.view(\n            Q.size(0), -1, self.d_model)\n\n        output = self.W_o(scaled_attention)\n        return output, attention_weights\n"})}),"\n",(0,s.jsx)(n.h2,{id:"perception-pipelines-in-robotics",children:"Perception Pipelines in Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"visual-feature-extraction",children:"Visual Feature Extraction"}),"\n",(0,s.jsx)(n.p,{children:"In robotic applications, VLMs must process visual input from cameras and sensors. The perception pipeline typically includes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Acquisition"}),": Capturing images from robot-mounted cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Preprocessing"}),": Resizing, normalization, and augmentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Extraction"}),": Converting images to patch embeddings"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Contextual Processing"}),": Applying transformer layers to extract meaningful representations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"language-processing-for-robotics",children:"Language Processing for Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Robotics applications require specialized language processing that connects linguistic concepts to physical actions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spatial Language Understanding"}),": Processing prepositions, directions, and spatial relationships"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Verb Recognition"}),": Identifying verbs that correspond to robot capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Reference Resolution"}),": Connecting pronouns and demonstratives to specific objects"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-fusion-for-action-grounding",children:"Multimodal Fusion for Action Grounding"}),"\n",(0,s.jsx)(n.p,{children:"The final stage of the VLM pipeline involves fusing visual and linguistic information to create representations suitable for action planning:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\n\nclass MultimodalFusion(nn.Module):\n    def __init__(self, vision_dim, text_dim, fusion_dim):\n        super().__init__()\n        self.vision_project = nn.Linear(vision_dim, fusion_dim)\n        self.text_project = nn.Linear(text_dim, fusion_dim)\n        self.fusion_transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=fusion_dim, nhead=8),\n            num_layers=2\n        )\n        self.output_head = nn.Linear(fusion_dim, fusion_dim)\n\n    def forward(self, vision_features, text_features):\n        # Project both modalities to shared space\n        vision_proj = self.vision_project(vision_features)\n        text_proj = self.text_project(text_features)\n\n        # Concatenate modalities\n        combined_features = torch.cat([vision_proj, text_proj], dim=1)\n\n        # Apply fusion transformer\n        fused_features = self.fusion_transformer(combined_features)\n\n        # Extract output\n        output = self.output_head(fused_features)\n        return output\n\n# Example usage in a robot perception system\nclass RobotVLM(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.vision_encoder = VisionTransformer(\n            image_size=224, patch_size=16, num_classes=1000,\n            dim=768, depth=12, heads=12, mlp_dim=3072\n        )\n        self.text_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=512, nhead=8),\n            num_layers=6\n        )\n        self.fusion_module = MultimodalFusion(\n            vision_dim=768, text_dim=512, fusion_dim=1024\n        )\n\n    def forward(self, image, text_tokens):\n        # Process visual input\n        vision_features = self.vision_encoder(image)\n\n        # Process text input\n        text_features = self.text_encoder(text_tokens)\n\n        # Fuse modalities\n        fused_features = self.fusion_module(\n            vision_features.unsqueeze(1),  # Add sequence dimension\n            text_features\n        )\n\n        return fused_features\n"})}),"\n",(0,s.jsx)(n.h2,{id:"implementation-considerations-for-robotics",children:"Implementation Considerations for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"real-time-processing-requirements",children:"Real-Time Processing Requirements"}),"\n",(0,s.jsx)(n.p,{children:"Robotic applications demand real-time processing capabilities. Key considerations include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Optimization"}),": Using techniques like quantization and pruning to reduce computational requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficient Architectures"}),": Selecting lightweight models suitable for embedded deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Processing"}),": Optimizing for the specific batch sizes required by robotic applications"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"memory-and-computation-constraints",children:"Memory and Computation Constraints"}),"\n",(0,s.jsx)(n.p,{children:"Robots often operate with limited computational resources. Efficient implementation strategies include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Incremental Processing"}),": Processing visual streams incrementally rather than in large batches"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Caching"}),": Caching intermediate features to avoid redundant computation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Selective Attention"}),": Focusing computational resources on relevant regions of interest"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"robustness-and-safety",children:"Robustness and Safety"}),"\n",(0,s.jsx)(n.p,{children:"VLMs in robotic applications must be robust to environmental variations and safe in their behavior:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Uncertainty Estimation"}),": Quantifying model confidence to enable safe fallback behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adversarial Robustness"}),": Ensuring model performance under distribution shift"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Constraints"}),": Integrating safety checks with model outputs"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,s.jsx)(n.h3,{id:"message-types-for-vlm-outputs",children:"Message Types for VLM Outputs"}),"\n",(0,s.jsx)(n.p,{children:"VLM outputs can be integrated with ROS 2 using custom message types:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example ROS 2 message definition for multimodal features\n# In msg/MultimodalFeatures.msg:\n# float32[] vision_features\n# float32[] text_features\n# float32[] fused_features\n# string[] detected_objects\n# float32[] object_positions\n"})}),"\n",(0,s.jsx)(n.h3,{id:"publisher-implementation",children:"Publisher Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom your_package.msg import MultimodalFeatures  # Custom message type\nfrom cv_bridge import CvBridge\nimport torch\n\nclass VLMPublisher(Node):\n    def __init__(self):\n        super().__init__('vlm_publisher')\n        self.bridge = CvBridge()\n        self.vlm_model = RobotVLM()  # Pre-trained model\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n        self.text_sub = self.create_subscription(\n            String, 'command_text', self.text_callback, 10)\n        self.features_pub = self.create_publisher(\n            MultimodalFeatures, 'multimodal_features', 10)\n\n        self.latest_image = None\n        self.latest_text = None\n\n    def image_callback(self, msg):\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n        # Process image through VLM\n        vision_features = self.process_image(cv_image)\n        self.latest_image = vision_features\n\n    def text_callback(self, msg):\n        # Process text through VLM\n        text_features = self.process_text(msg.data)\n        self.latest_text = text_features\n\n    def process_image(self, image):\n        # Convert image to tensor and process\n        image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()\n        vision_features = self.vlm_model.vision_encoder(image_tensor)\n        return vision_features\n\n    def process_text(self, text):\n        # Tokenize and encode text\n        tokens = self.tokenize(text)\n        text_tensor = torch.tensor(tokens).unsqueeze(0)\n        text_features = self.vlm_model.text_encoder(text_tensor)\n        return text_features\n"})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Vision-Language Models provide the essential capability for VLA systems to connect visual perception with natural language understanding. Through transformer architectures and attention mechanisms, these models create unified representations that enable humanoid robots to interpret complex visual scenes and understand human instructions. The integration of VLMs with robotic systems requires careful consideration of real-time processing requirements, memory constraints, and safety considerations. In the next chapter, we will explore how these perception capabilities connect to action planning and execution in robotic systems."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);