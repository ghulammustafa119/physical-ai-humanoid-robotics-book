"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[911],{4117(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var o=t(4848),i=t(8453);const a={title:"Chapter 3: Action Planning & Execution",description:"Connecting VLM outputs to robot actions with ROS 2 integration",sidebar_position:15},s="Chapter 3: Action Planning & Execution",r={id:"vla/chapters/chapter3",title:"Chapter 3: Action Planning & Execution",description:"Connecting VLM outputs to robot actions with ROS 2 integration",source:"@site/docs/vla/chapters/chapter3.md",sourceDirName:"vla/chapters",slug:"/vla/chapters/chapter3",permalink:"/physical-ai-humanoid-robotics-book/docs/vla/chapters/chapter3",draft:!1,unlisted:!1,editUrl:"https://github.com/ghulammustafa119/physical-ai-humanoid-robotics-book/tree/main/docs/vla/chapters/chapter3.md",tags:[],version:"current",sidebarPosition:15,frontMatter:{title:"Chapter 3: Action Planning & Execution",description:"Connecting VLM outputs to robot actions with ROS 2 integration",sidebar_position:15},sidebar:"tutorialSidebar",previous:{title:"Chapter 2: Vision-Language Models (VLMs)",permalink:"/physical-ai-humanoid-robotics-book/docs/vla/chapters/chapter2"},next:{title:"Chapter 4: Integration & Simulation",permalink:"/physical-ai-humanoid-robotics-book/docs/vla/chapters/chapter4"}},c={},l=[{value:"Overview",id:"overview",level:2},{value:"Decision-Making Frameworks",id:"decision-making-frameworks",level:2},{value:"From VLM Output to Action Space",id:"from-vlm-output-to-action-space",level:3},{value:"Hierarchical Action Planning",id:"hierarchical-action-planning",level:3},{value:"Task and Motion Planning Integration",id:"task-and-motion-planning-integration",level:3},{value:"Connecting VLM Outputs to Robot Actions",id:"connecting-vlm-outputs-to-robot-actions",level:2},{value:"Action Vocabulary Design",id:"action-vocabulary-design",level:3},{value:"Command Mapping Algorithms",id:"command-mapping-algorithms",level:3},{value:"Context-Aware Action Selection",id:"context-aware-action-selection",level:3},{value:"ROS 2 Integration for Action Execution",id:"ros-2-integration-for-action-execution",level:2},{value:"Action Server Architecture",id:"action-server-architecture",level:3},{value:"Action Client Implementation",id:"action-client-implementation",level:3},{value:"Feedback Loops and Adaptive Behavior",id:"feedback-loops-and-adaptive-behavior",level:2},{value:"Sensory Feedback Integration",id:"sensory-feedback-integration",level:3},{value:"Closed-Loop Control",id:"closed-loop-control",level:3},{value:"Adaptive Behavior Systems",id:"adaptive-behavior-systems",level:3},{value:"Safety and Error Handling",id:"safety-and-error-handling",level:2},{value:"Safety Constraints Implementation",id:"safety-constraints-implementation",level:3},{value:"Error Recovery Strategies",id:"error-recovery-strategies",level:3},{value:"Implementation Examples",id:"implementation-examples",level:2},{value:"Complete VLA Integration Node",id:"complete-vla-integration-node",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"chapter-3-action-planning--execution",children:"Chapter 3: Action Planning & Execution"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Action planning and execution form the critical bridge between the perception capabilities provided by Vision-Language Models (VLMs) and the physical manifestation of robot behavior. In Vision-Language-Action (VLA) systems, the action component must translate high-level linguistic commands and visual understanding into specific motor commands that achieve the desired goals while maintaining safety and efficiency."}),"\n",(0,o.jsx)(n.p,{children:"This chapter explores how VLA systems connect the rich multimodal understanding provided by VLMs to concrete robot actions, focusing on the integration with ROS 2 for reliable and standardized robot control. We'll examine decision-making frameworks, feedback mechanisms, and the practical implementation of action execution in humanoid robots."}),"\n",(0,o.jsx)(n.h2,{id:"decision-making-frameworks",children:"Decision-Making Frameworks"}),"\n",(0,o.jsx)(n.h3,{id:"from-vlm-output-to-action-space",children:"From VLM Output to Action Space"}),"\n",(0,o.jsx)(n.p,{children:"The transition from VLM output to executable actions involves several key steps:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Semantic Interpretation"}),": Converting VLM outputs into meaningful action concepts"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Space Mapping"}),": Translating high-level concepts to specific robot capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Constraint Validation"}),": Ensuring actions are feasible and safe"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Execution Planning"}),": Generating detailed motion plans for action execution"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The challenge lies in bridging the gap between the abstract representations learned by VLMs and the concrete physical actions that robots can perform. This requires careful design of the interface between perception and action systems."}),"\n",(0,o.jsx)(n.h3,{id:"hierarchical-action-planning",children:"Hierarchical Action Planning"}),"\n",(0,o.jsx)(n.p,{children:"VLA systems often employ hierarchical action planning to handle complex tasks:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High-Level Planning"}),": Decomposing complex goals into subtasks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Mid-Level Planning"}),": Generating sequences of primitive actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Low-Level Execution"}),": Controlling individual joints and actuators"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:'This hierarchy allows the system to handle both high-level commands like "set the table for dinner" and low-level precision tasks like "carefully place the wine glass on the table."'}),"\n",(0,o.jsx)(n.h3,{id:"task-and-motion-planning-integration",children:"Task and Motion Planning Integration"}),"\n",(0,o.jsx)(n.p,{children:"Modern VLA systems integrate task planning (what to do) with motion planning (how to do it) to create coherent action sequences. This integration considers:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Geometric Constraints"}),": Physical reachability and collision avoidance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Temporal Constraints"}),": Sequencing of actions over time"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Constraints"}),": Energy, time, and computational limitations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environmental Constraints"}),": Dynamic obstacles and changing conditions"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"connecting-vlm-outputs-to-robot-actions",children:"Connecting VLM Outputs to Robot Actions"}),"\n",(0,o.jsx)(n.h3,{id:"action-vocabulary-design",children:"Action Vocabulary Design"}),"\n",(0,o.jsx)(n.p,{children:"The action vocabulary defines the set of actions that the robot can perform and how they connect to VLM outputs. A well-designed vocabulary includes:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Basic Motion Primitives"}),": Move, turn, approach, retreat"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation Actions"}),": Grasp, release, push, pull"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation Actions"}),": Go to location, follow path, avoid obstacles"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Social Actions"}),": Wave, nod, maintain eye contact"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The vocabulary should be designed to match both the linguistic concepts that VLMs can understand and the physical capabilities of the humanoid robot."}),"\n",(0,o.jsx)(n.h3,{id:"command-mapping-algorithms",children:"Command Mapping Algorithms"}),"\n",(0,o.jsx)(n.p,{children:"Mapping natural language commands to robot actions requires sophisticated algorithms that handle ambiguity and context:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import re\nfrom typing import Dict, List, Tuple, Optional\n\nclass ActionCommandMapper:\n    def __init__(self):\n        # Define action vocabulary\n        self.action_patterns = {\n            'move_forward': [\n                r'move forward (\\d+(?:\\.\\d+)?)\\s*(m|meter|cm|centimeter)',\n                r'go forward (\\d+(?:\\.\\d+)?)\\s*(m|meter|cm|centimeter)',\n                r'walk forward (\\d+(?:\\.\\d+)?)\\s*(m|meter|cm|centimeter)'\n            ],\n            'turn_left': [\n                r'turn left (\\d+(?:\\.\\d+)?)\\s*(deg|degree)',\n                r'turn counterclockwise (\\d+(?:\\.\\d+)?)\\s*(deg|degree)',\n                r'rotate left (\\d+(?:\\.\\d+)?)\\s*(deg|degree)'\n            ],\n            'grasp': [\n                r'pick up (.+)',\n                r'grab (.+)',\n                r'take (.+)',\n                r'pick (.+)'\n            ],\n            'place': [\n                r'place (.+) on (.+)',\n                r'put (.+) on (.+)',\n                r'place (.+) at (.+)'\n            ],\n            'navigate': [\n                r'go to (.+)',\n                r'move to (.+)',\n                r'go near (.+)',\n                r'approach (.+)'\n            ]\n        }\n\n    def parse_command(self, command: str) -> Dict:\n        \"\"\"Parse a natural language command into structured action.\"\"\"\n        command_lower = command.lower().strip()\n\n        for action_type, patterns in self.action_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, command_lower, re.IGNORECASE)\n                if match:\n                    params = list(match.groups())\n                    return {\n                        'action_type': action_type,\n                        'parameters': params,\n                        'confidence': 0.9  # Placeholder confidence\n                    }\n\n        # If no specific pattern matches, return a generic response\n        return {\n            'action_type': 'unknown',\n            'parameters': [command],\n            'confidence': 0.1\n        }\n\n    def refine_with_vlm_context(self, vlm_output: Dict, parsed_command: Dict) -> Dict:\n        \"\"\"Refine action based on VLM understanding of the scene.\"\"\"\n        # Use VLM output to disambiguate commands\n        if parsed_command['action_type'] == 'grasp':\n            if len(parsed_command['parameters']) == 1:\n                target_obj = parsed_command['parameters'][0]\n                # Use VLM to identify specific object in scene\n                if 'objects' in vlm_output:\n                    for obj in vlm_output['objects']:\n                        if target_obj.lower() in obj['name'].lower():\n                            parsed_command['parameters'].append(obj['position'])\n                            break\n\n        return parsed_command\n"})}),"\n",(0,o.jsx)(n.h3,{id:"context-aware-action-selection",children:"Context-Aware Action Selection"}),"\n",(0,o.jsx)(n.p,{children:"VLA systems must consider contextual information when selecting actions:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environmental Context"}),": Current scene layout, object positions, and obstacles"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Social Context"}),": Human presence, social norms, and interaction protocols"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Context"}),": Current task state, previous actions, and expected outcomes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Physical Context"}),": Robot state, battery level, and available resources"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"ros-2-integration-for-action-execution",children:"ROS 2 Integration for Action Execution"}),"\n",(0,o.jsx)(n.h3,{id:"action-server-architecture",children:"Action Server Architecture"}),"\n",(0,o.jsx)(n.p,{children:"ROS 2 provides a robust framework for action execution through action servers that handle long-running tasks with feedback and goal management:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\n\n# Example action definition (this would be defined in .action files)\n# In practice, you would define your own action types like:\n# action/VLAAction.action\n# string action_type\n# string[] parameters\n# ---\n# bool success\n# string message\n# ---\n# float32 progress\n# string status\n\nclass VLAActionServer(Node):\n    def __init__(self):\n        super().__init__('vla_action_server')\n\n        # Create action server with reentrant callback group for concurrent handling\n        self._action_server = ActionServer(\n            self,\n            # VLAAction,  # Your custom action type\n            'vla_execute_action',\n            self.execute_callback,\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback,\n            callback_group=ReentrantCallbackGroup()\n        )\n\n        # Robot control interfaces\n        self.robot_controller = RobotController()\n        self.get_logger().info('VLA Action Server initialized')\n\n    def goal_callback(self, goal_request):\n        \"\"\"Accept or reject a client request to begin an action.\"\"\"\n        self.get_logger().info(f'Received goal request: {goal_request.action_type}')\n\n        # Validate goal feasibility\n        if self.is_goal_feasible(goal_request):\n            return GoalResponse.ACCEPT\n        else:\n            return GoalResponse.REJECT\n\n    def cancel_callback(self, goal_handle):\n        \"\"\"Accept or reject a client request to cancel an action.\"\"\"\n        self.get_logger().info('Received cancel request')\n        return CancelResponse.ACCEPT\n\n    def execute_callback(self, goal_handle):\n        \"\"\"Execute the goal.\"\"\"\n        self.get_logger().info('Executing goal...')\n\n        # Initialize feedback\n        feedback_msg = None  # VLAAction.Feedback()\n        feedback_msg.progress = 0.0\n        feedback_msg.status = \"Initializing\"\n\n        # Execute the action based on goal request\n        action_result = self.execute_action(goal_handle.request, goal_handle)\n\n        # Determine success based on action execution\n        goal_handle.succeed()\n\n        result = None  # VLAAction.Result()\n        result.success = action_result['success']\n        result.message = action_result['message']\n\n        self.get_logger().info(f'Returning result: {result.success}')\n        return result\n\n    def is_goal_feasible(self, goal_request):\n        \"\"\"Check if the requested action is feasible.\"\"\"\n        # Implement feasibility checks\n        return True\n\n    def execute_action(self, request, goal_handle):\n        \"\"\"Execute the specific action based on request parameters.\"\"\"\n        action_type = request.action_type\n        params = request.parameters\n\n        try:\n            if action_type == 'move_forward':\n                return self.move_forward(float(params[0]) if params else 1.0, goal_handle)\n            elif action_type == 'turn_left':\n                return self.turn_left(float(params[0]) if params else 90.0, goal_handle)\n            elif action_type == 'grasp':\n                return self.grasp_object(params[0] if params else 'object', goal_handle)\n            elif action_type == 'place':\n                return self.place_object(params[0] if len(params) > 0 else 'object',\n                                       params[1] if len(params) > 1 else 'table', goal_handle)\n            elif action_type == 'navigate':\n                return self.navigate_to_location(params[0] if params else 'location', goal_handle)\n            else:\n                return {'success': False, 'message': f'Unknown action: {action_type}'}\n        except Exception as e:\n            self.get_logger().error(f'Error executing action: {e}')\n            return {'success': False, 'message': f'Execution error: {str(e)}'}\n\n    def move_forward(self, distance, goal_handle):\n        \"\"\"Execute forward movement.\"\"\"\n        self.get_logger().info(f'Moving forward {distance} meters')\n        # Implement actual movement logic\n        return {'success': True, 'message': f'Moved forward {distance} meters'}\n\n    def turn_left(self, angle, goal_handle):\n        \"\"\"Execute left turn.\"\"\"\n        self.get_logger().info(f'Turning left {angle} degrees')\n        # Implement actual turning logic\n        return {'success': True, 'message': f'Turned left {angle} degrees'}\n\n    def grasp_object(self, object_name, goal_handle):\n        \"\"\"Execute grasping action.\"\"\"\n        self.get_logger().info(f'Attempting to grasp {object_name}')\n        # Implement actual grasping logic\n        return {'success': True, 'message': f'Grasped {object_name}'}\n\n    def place_object(self, object_name, location, goal_handle):\n        \"\"\"Execute placing action.\"\"\"\n        self.get_logger().info(f'Placing {object_name} at {location}')\n        # Implement actual placing logic\n        return {'success': True, 'message': f'Placed {object_name} at {location}'}\n\n    def navigate_to_location(self, location, goal_handle):\n        \"\"\"Execute navigation to location.\"\"\"\n        self.get_logger().info(f'Navigating to {location}')\n        # Implement actual navigation logic\n        return {'success': True, 'message': f'Navigated to {location}'}\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    vla_action_server = VLAActionServer()\n\n    # Use multi-threaded executor to handle multiple concurrent actions\n    executor = MultiThreadedExecutor()\n    rclpy.spin(vla_action_server, executor=executor)\n\n    vla_action_server.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"action-client-implementation",children:"Action Client Implementation"}),"\n",(0,o.jsx)(n.p,{children:"Action clients allow other nodes to request action execution:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nimport time\n\nclass VLAActionClient(Node):\n    def __init__(self):\n        super().__init__(\'vla_action_client\')\n        self._action_client = ActionClient(\n            self,\n            # VLAAction,  # Your custom action type\n            \'vla_execute_action\'\n        )\n\n    def send_action_goal(self, action_type, parameters):\n        """Send an action goal to the server."""\n        goal_msg = None  # VLAAction.Goal()\n        goal_msg.action_type = action_type\n        goal_msg.parameters = parameters\n\n        self.get_logger().info(f\'Waiting for action server...\')\n        self._action_client.wait_for_server()\n\n        self.get_logger().info(f\'Sending goal: {action_type} with params {parameters}\')\n        send_goal_future = self._action_client.send_goal_async(\n            goal_msg,\n            feedback_callback=self.feedback_callback\n        )\n\n        send_goal_future.add_done_callback(self.goal_response_callback)\n\n    def goal_response_callback(self, future):\n        """Handle goal response."""\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info(\'Goal rejected\')\n            return\n\n        self.get_logger().info(\'Goal accepted\')\n        get_result_future = goal_handle.get_result_async()\n        get_result_future.add_done_callback(self.get_result_callback)\n\n    def get_result_callback(self, future):\n        """Handle action result."""\n        result = future.result().result\n        self.get_logger().info(f\'Result: {result.success}, {result.message}\')\n\n    def feedback_callback(self, feedback_msg):\n        """Handle action feedback."""\n        feedback = feedback_msg.feedback\n        self.get_logger().info(f\'Feedback: {feedback.status}, {feedback.progress:.2f}\')\n'})}),"\n",(0,o.jsx)(n.h2,{id:"feedback-loops-and-adaptive-behavior",children:"Feedback Loops and Adaptive Behavior"}),"\n",(0,o.jsx)(n.h3,{id:"sensory-feedback-integration",children:"Sensory Feedback Integration"}),"\n",(0,o.jsx)(n.p,{children:"VLA systems must continuously monitor action execution and adapt based on sensory feedback:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Visual Feedback"}),": Confirming object positions, grasp success, and action outcomes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Proprioceptive Feedback"}),": Monitoring joint positions, forces, and robot state"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tactile Feedback"}),": Detecting contact, slip, and manipulation success"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Feedback"}),": Detecting environmental changes and human responses"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"closed-loop-control",children:"Closed-Loop Control"}),"\n",(0,o.jsx)(n.p,{children:"Closed-loop control ensures that actions adapt to changing conditions:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import threading\nimport time\nfrom collections import deque\n\nclass ClosedLoopController:\n    def __init__(self, robot_controller, perception_system):\n        self.robot_controller = robot_controller\n        self.perception_system = perception_system\n        self.control_thread = None\n        self.running = False\n        self.feedback_buffer = deque(maxlen=10)  # Store recent feedback\n\n    def start_control_loop(self, action_plan):\n        """Start closed-loop control for an action plan."""\n        self.running = True\n        self.control_thread = threading.Thread(\n            target=self.control_loop,\n            args=(action_plan,)\n        )\n        self.control_thread.start()\n\n    def control_loop(self, action_plan):\n        """Main control loop with feedback integration."""\n        for action in action_plan:\n            if not self.running:\n                break\n\n            # Execute action\n            success = self.execute_action_with_monitoring(action)\n\n            if not success:\n                # Handle failure by replanning or recovery\n                self.handle_action_failure(action)\n\n            # Update perception for next iteration\n            self.update_perception()\n\n    def execute_action_with_monitoring(self, action):\n        """Execute action while monitoring for success/failure."""\n        start_time = time.time()\n\n        # Start action execution\n        self.robot_controller.execute_action(action)\n\n        # Monitor execution with feedback\n        while self.robot_controller.is_executing():\n            if not self.running:\n                return False\n\n            # Get current state\n            current_state = self.robot_controller.get_state()\n            perceived_state = self.perception_system.get_perception()\n\n            # Check for success conditions\n            if self.check_success_conditions(action, current_state, perceived_state):\n                return True\n\n            # Check for failure conditions\n            if self.check_failure_conditions(action, current_state, perceived_state):\n                return False\n\n            # Adjust action based on feedback\n            self.adjust_action(action, current_state, perceived_state)\n\n            time.sleep(0.1)  # Control loop frequency\n\n        return True  # Assume success if execution completes\n\n    def check_success_conditions(self, action, current_state, perceived_state):\n        """Check if action has been successfully completed."""\n        # Implementation depends on action type\n        return False\n\n    def check_failure_conditions(self, action, current_state, perceived_state):\n        """Check if action has failed."""\n        # Implementation depends on action type\n        return False\n\n    def adjust_action(self, action, current_state, perceived_state):\n        """Adjust action based on current state and perception."""\n        # Implementation depends on action type\n        pass\n\n    def handle_action_failure(self, failed_action):\n        """Handle action failure with recovery strategies."""\n        # Implement recovery logic\n        pass\n\n    def update_perception(self):\n        """Update perception system for current state."""\n        self.perception_system.update()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"adaptive-behavior-systems",children:"Adaptive Behavior Systems"}),"\n",(0,o.jsx)(n.p,{children:"Adaptive behavior allows VLA systems to adjust their actions based on experience and environmental changes:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Learning from Experience"}),": Improving action selection based on past outcomes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Context Adaptation"}),": Modifying behavior based on environmental conditions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human Preference Learning"}),": Adapting to individual user preferences"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety Adaptation"}),": Adjusting behavior based on risk assessment"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"safety-and-error-handling",children:"Safety and Error Handling"}),"\n",(0,o.jsx)(n.h3,{id:"safety-constraints-implementation",children:"Safety Constraints Implementation"}),"\n",(0,o.jsx)(n.p,{children:"Safety is paramount in VLA systems, especially when controlling humanoid robots around humans:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class SafetyManager:\n    def __init__(self):\n        self.safety_constraints = {\n            \'collision_avoidance\': True,\n            \'force_limits\': True,\n            \'workspace_bounds\': True,\n            \'human_proximity\': True\n        }\n        self.emergency_stop = False\n\n    def validate_action(self, action):\n        """Validate action against safety constraints."""\n        if self.emergency_stop:\n            return False, "Emergency stop active"\n\n        # Check collision constraints\n        if self.safety_constraints[\'collision_avoidance\']:\n            collision_risk = self.check_collision_risk(action)\n            if collision_risk > 0.9:  # High risk threshold\n                return False, "High collision risk"\n\n        # Check force limits\n        if self.safety_constraints[\'force_limits\']:\n            force_exceeded = self.check_force_limits(action)\n            if force_exceeded:\n                return False, "Force limits exceeded"\n\n        # Check workspace bounds\n        if self.safety_constraints[\'workspace_bounds\']:\n            out_of_bounds = self.check_workspace_bounds(action)\n            if out_of_bounds:\n                return False, "Action would exceed workspace bounds"\n\n        # Check human proximity\n        if self.safety_constraints[\'human_proximity\']:\n            human_too_close = self.check_human_proximity(action)\n            if human_too_close:\n                return False, "Action would bring robot too close to human"\n\n        return True, "Action is safe"\n\n    def check_collision_risk(self, action):\n        """Check for potential collisions."""\n        # Implementation would use motion planning and collision detection\n        return 0.0\n\n    def check_force_limits(self, action):\n        """Check if action would exceed force limits."""\n        # Implementation would check expected forces\n        return False\n\n    def check_workspace_bounds(self, action):\n        """Check if action would exceed workspace bounds."""\n        # Implementation would check joint limits and workspace\n        return False\n\n    def check_human_proximity(self, action):\n        """Check if action would bring robot too close to humans."""\n        # Implementation would use human detection and tracking\n        return False\n\n    def trigger_emergency_stop(self):\n        """Trigger emergency stop for immediate safety."""\n        self.emergency_stop = True\n        # Implement immediate robot stop\n        self.stop_robot_immediately()\n\n    def stop_robot_immediately(self):\n        """Stop all robot motion immediately."""\n        # Implementation would send immediate stop commands\n        pass\n'})}),"\n",(0,o.jsx)(n.h3,{id:"error-recovery-strategies",children:"Error Recovery Strategies"}),"\n",(0,o.jsx)(n.p,{children:"VLA systems must implement robust error recovery to handle failures gracefully:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Graceful Degradation"}),": Continuing operation with reduced capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Fallback Behaviors"}),": Using alternative methods when primary approach fails"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human Intervention"}),": Requesting human assistance when needed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safe State Recovery"}),": Returning to a safe state when errors occur"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"implementation-examples",children:"Implementation Examples"}),"\n",(0,o.jsx)(n.h3,{id:"complete-vla-integration-node",children:"Complete VLA Integration Node"}),"\n",(0,o.jsx)(n.p,{children:"Here's an example of a complete node that integrates VLM perception with action execution:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom cv_bridge import CvBridge\nimport threading\nimport time\n\nclass VLAIntegratedNode(Node):\n    def __init__(self):\n        super().__init__(\'vla_integrated_node\')\n        self.bridge = CvBridge()\n\n        # Initialize components\n        self.vlm_model = None  # Initialize your VLM model\n        self.action_mapper = ActionCommandMapper()\n        self.action_server = VLAActionServer(self)  # Embedded action server\n        self.safety_manager = SafetyManager()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, 10)\n        self.command_sub = self.create_subscription(\n            String, \'vla_command\', self.command_callback, 10)\n        self.vision_pub = self.create_publisher(\n            String, \'vla_vision_output\', 10)\n        self.action_pub = self.create_publisher(\n            String, \'vla_action_output\', 10)\n\n        # Internal state\n        self.latest_image = None\n        self.latest_command = None\n        self.system_active = True\n        self.processing_lock = threading.Lock()\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_loop)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        self.get_logger().info(\'VLA Integrated Node initialized\')\n\n    def image_callback(self, msg):\n        """Handle incoming image data."""\n        with self.processing_lock:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.latest_image = cv_image\n\n    def command_callback(self, msg):\n        """Handle incoming command data."""\n        with self.processing_lock:\n            self.latest_command = msg.data\n            self.get_logger().info(f\'Received command: {msg.data}\')\n\n    def process_loop(self):\n        """Main processing loop for VLA integration."""\n        while self.system_active:\n            with self.processing_lock:\n                if self.latest_image is not None and self.latest_command is not None:\n                    # Process VLA pipeline\n                    self.process_vla_input(\n                        self.latest_image,\n                        self.latest_command\n                    )\n\n                    # Reset processed inputs\n                    self.latest_image = None\n                    self.latest_command = None\n\n            time.sleep(0.1)  # Processing frequency\n\n    def process_vla_input(self, image, command):\n        """Process combined vision-language input to generate action."""\n        try:\n            # Step 1: Process visual input with VLM\n            vlm_output = self.process_visual_input(image)\n\n            # Step 2: Parse command with action mapper\n            parsed_command = self.action_mapper.parse_command(command)\n\n            # Step 3: Refine with VLM context\n            refined_command = self.action_mapper.refine_with_vlm_context(\n                vlm_output, parsed_command\n            )\n\n            # Step 4: Validate with safety manager\n            is_safe, safety_msg = self.safety_manager.validate_action(refined_command)\n            if not is_safe:\n                self.get_logger().warn(f\'Safety check failed: {safety_msg}\')\n                return\n\n            # Step 5: Execute action\n            self.execute_action(refined_command)\n\n            # Log the complete pipeline\n            self.get_logger().info(\n                f\'VLA Pipeline: "{command}" -> {refined_command["action_type"]} \'\n                f\'with confidence {refined_command["confidence"]:.2f}\'\n            )\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in VLA processing: {e}\')\n\n    def process_visual_input(self, image):\n        """Process visual input through VLM."""\n        # Placeholder for VLM processing\n        # In practice, this would run the image through your VLM model\n        return {\n            \'objects\': [{\'name\': \'test_object\', \'position\': [0, 0, 0]}],\n            \'scene_description\': \'test scene\'\n        }\n\n    def execute_action(self, action_command):\n        """Execute the action command."""\n        # Publish action for execution\n        action_msg = String()\n        action_msg.data = str(action_command)\n        self.action_pub.publish(action_msg)\n\n    def destroy_node(self):\n        """Clean up before destroying node."""\n        self.system_active = False\n        if self.processing_thread.is_alive():\n            self.processing_thread.join()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    vla_node = VLAIntegratedNode()\n\n    try:\n        rclpy.spin(vla_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vla_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"This chapter explored the critical connection between Vision-Language Models and robot action execution in VLA systems. We examined decision-making frameworks, the mapping of VLM outputs to robot actions, and the integration with ROS 2 for reliable action execution. The chapter covered feedback loops, adaptive behavior systems, and essential safety considerations for deploying VLA systems with humanoid robots."}),"\n",(0,o.jsx)(n.p,{children:"In the next chapter, we will focus on the integration of all VLA components in simulation environments, evaluation metrics, and the implementation of complete VLA systems for humanoid robots."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);